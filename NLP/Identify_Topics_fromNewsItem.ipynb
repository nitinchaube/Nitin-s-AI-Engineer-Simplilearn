{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "binding-primary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nitin\n"
     ]
    }
   ],
   "source": [
    "print(\"nitin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "clean-landscape",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroup_train= fetch_20newsgroups(subset=\"train\",shuffle=True)\n",
    "newsgroup_test = fetch_20newsgroups(subset=\"test\",shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cleared-interference",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "print(list(newsgroup_train.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "numerous-agreement",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['From: v064mb9k@ubvmsd.cc.buffalo.edu (NEIL B. GANDLER)\\nSubject: Need info on 88-89 Bonneville\\nOrganization: University at Buffalo\\nLines: 10\\nNews-Software: VAX/VMS VNEWS 1.41\\nNntp-Posting-Host: ubvmsd.cc.buffalo.edu\\n\\n\\n I am a little confused on all of the models of the 88-89 bonnevilles.\\nI have heard of the LE SE LSE SSE SSEI. Could someone tell me the\\ndifferences are far as features or performance. I am also curious to\\nknow what the book value is for prefereably the 89 model. And how much\\nless than book value can you usually get them for. In other words how\\nmuch are they in demand this time of year. I have heard that the mid-spring\\nearly summer is the best time to buy.\\n\\n\\t\\t\\tNeil Gandler\\n',\n",
       " 'From: Rick Miller <rick@ee.uwm.edu>\\nSubject: X-Face?\\nOrganization: Just me.\\nLines: 17\\nDistribution: world\\nNNTP-Posting-Host: 129.89.2.33\\nSummary: Go ahead... swamp me.  <EEP!>\\n\\nI\\'m not familiar at all with the format of these \"X-Face:\" thingies, but\\nafter seeing them in some folks\\' headers, I\\'ve *got* to *see* them (and\\nmaybe make one of my own)!\\n\\nI\\'ve got \"dpg-view\" on my Linux box (which displays \"uncompressed X-Faces\")\\nand I\\'ve managed to compile [un]compface too... but now that I\\'m *looking*\\nfor them, I can\\'t seem to find any X-Face:\\'s in anyones news headers!  :-(\\n\\nCould you, would you, please send me your \"X-Face:\" header?\\n\\nI *know* I\\'ll probably get a little swamped, but I can handle it.\\n\\n\\t...I hope.\\n\\nRick Miller  <rick@ee.uwm.edu> | <ricxjo@discus.mil.wi.us>   Ricxjo Muelisto\\nSend a postcard, get one back! | Enposxtigu bildkarton kaj vi ricevos alion!\\n          RICK MILLER // 16203 WOODS // MUSKEGO, WIS. 53150 // USA\\n']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroup_test.data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "brazilian-saying",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim \n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "excess-magic",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nitin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "radio-lemon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "study\n"
     ]
    }
   ],
   "source": [
    "print(WordNetLemmatizer().lemmatize(\"studying\",pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aware-better",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text,pos='v'))\n",
    "\n",
    "#tokenize and lemmatize\n",
    "def preprocess(text):\n",
    "    result=[]\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in STOPWORDS and len(token) >3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "renewable-conservation",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs=[]\n",
    "for doc in newsgroup_train.data:\n",
    "    processed_docs.append(preprocess(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "right-start",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['lerxst',\n",
       "  'thing',\n",
       "  'subject',\n",
       "  'nntp',\n",
       "  'post',\n",
       "  'host',\n",
       "  'organ',\n",
       "  'univers',\n",
       "  'maryland',\n",
       "  'colleg',\n",
       "  'park',\n",
       "  'line',\n",
       "  'wonder',\n",
       "  'enlighten',\n",
       "  'door',\n",
       "  'sport',\n",
       "  'look',\n",
       "  'late',\n",
       "  'earli',\n",
       "  'call',\n",
       "  'bricklin',\n",
       "  'door',\n",
       "  'small',\n",
       "  'addit',\n",
       "  'bumper',\n",
       "  'separ',\n",
       "  'rest',\n",
       "  'bodi',\n",
       "  'know',\n",
       "  'tellm',\n",
       "  'model',\n",
       "  'engin',\n",
       "  'spec',\n",
       "  'year',\n",
       "  'product',\n",
       "  'histori',\n",
       "  'info',\n",
       "  'funki',\n",
       "  'look',\n",
       "  'mail',\n",
       "  'thank',\n",
       "  'bring',\n",
       "  'neighborhood',\n",
       "  'lerxst'],\n",
       " ['guykuo',\n",
       "  'carson',\n",
       "  'washington',\n",
       "  'subject',\n",
       "  'clock',\n",
       "  'poll',\n",
       "  'final',\n",
       "  'summari',\n",
       "  'final',\n",
       "  'clock',\n",
       "  'report',\n",
       "  'keyword',\n",
       "  'acceler',\n",
       "  'clock',\n",
       "  'upgrad',\n",
       "  'articl',\n",
       "  'shelley',\n",
       "  'qvfo',\n",
       "  'innc',\n",
       "  'organ',\n",
       "  'univers',\n",
       "  'washington',\n",
       "  'line',\n",
       "  'nntp',\n",
       "  'post',\n",
       "  'host',\n",
       "  'carson',\n",
       "  'washington',\n",
       "  'fair',\n",
       "  'number',\n",
       "  'brave',\n",
       "  'soul',\n",
       "  'upgrad',\n",
       "  'clock',\n",
       "  'oscil',\n",
       "  'share',\n",
       "  'experi',\n",
       "  'poll',\n",
       "  'send',\n",
       "  'brief',\n",
       "  'messag',\n",
       "  'detail',\n",
       "  'experi',\n",
       "  'procedur',\n",
       "  'speed',\n",
       "  'attain',\n",
       "  'rat',\n",
       "  'speed',\n",
       "  'card',\n",
       "  'adapt',\n",
       "  'heat',\n",
       "  'sink',\n",
       "  'hour',\n",
       "  'usag',\n",
       "  'floppi',\n",
       "  'disk',\n",
       "  'function',\n",
       "  'floppi',\n",
       "  'especi',\n",
       "  'request',\n",
       "  'summar',\n",
       "  'day',\n",
       "  'network',\n",
       "  'knowledg',\n",
       "  'base',\n",
       "  'clock',\n",
       "  'upgrad',\n",
       "  'haven',\n",
       "  'answer',\n",
       "  'poll',\n",
       "  'thank',\n",
       "  'guykuo',\n",
       "  'washington']]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "going-stroke",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "antique-massachusetts",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.corpora.dictionary.Dictionary at 0x2421ba76790>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "widespread-corpus",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=15, no_above=0.1, keep_n=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reserved-university",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
