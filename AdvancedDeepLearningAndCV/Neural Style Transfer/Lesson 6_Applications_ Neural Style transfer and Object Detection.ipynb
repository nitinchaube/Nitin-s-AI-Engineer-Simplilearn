{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KmlaUuxRUwsL"
   },
   "source": [
    "# **Object Detection Using CNN**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_HbHXv0JuxJ8"
   },
   "source": [
    "## **Learning Objectives**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6I5EqSlcuxUk"
   },
   "source": [
    "By the end of this lesson you will be able to:<br>\n",
    "* Describe neural style transfer\n",
    "* Extract content from an image\n",
    "* Extract style from an image\n",
    "* Perform neural style transfer using TensorFlow and PyTorch\n",
    "* Implement CNN architecture\n",
    "* Use the concept of transfer learning to model complex networks\n",
    "* Explain object detection with YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dHeatfPV7_dy"
   },
   "source": [
    "### Neural Style Transfer:\n",
    "Neural Style Transfer uses a pretrained convolutional neural network (CNN) to transfer styles from a given image to another. \n",
    "\n",
    "It is achieved by a group of algorithms that takes two images, one content image and one style image. As an output, this algorithm combines the content and the style of the inputs to generate a totally new image.\n",
    "\n",
    "Consider two images A and B. C is the style transfered image. \n",
    "\n",
    "* A = Gothic style image (style image)\n",
    "* B = Rottweiler puppy image (content image)\n",
    "* C = Rottweiler puppy image with gothic style (newly generated image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m_7ynDhq8A0R"
   },
   "source": [
    "![alt text](https://miro.medium.com/max/1208/1*fxJGoLMvVTvbNf11eqK62w.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jeJYLfGz8HYo"
   },
   "source": [
    "### Convolutional Network of Neural Style Transfer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rXYZew-z8Iq-"
   },
   "source": [
    "## Extraction of Content from an Image:\n",
    "* Shape and design are the contents of an image.\n",
    "* As the convolution network progresses, the input image is transformed into a representation that increasingly considers the actual content pixel of the image compared to the detailed pixel value.\n",
    "* For content representation, the feature of the higher layers of the networks are referred.\n",
    "* The 4_2 convolution layer (second part of the fourth convolution layer) of pretrained VGG-19 network is used as a content extractor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4IR1Y9Lq8Psb"
   },
   "source": [
    "## Extraction of Style from an Image:\n",
    "* Color and texture are the styles of an image.\n",
    "* By performing correlation among various filter responses, the actual global arrangement of the pixels is ignored. Only the color and textures are extracted in other words only the styles are extracted.\n",
    "* The correlation between the feature maps are called the gram matrix.\n",
    "* For VGG-19 network, 1_1, 2_1, 3_1, 4_1, and 5_1 layers are used to calculate the gram matrix. \n",
    "* The style weight constant varies for each layer and can be seen as the hyperparameter used for changing the style level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2LiwSvrw8XBF"
   },
   "source": [
    "## Model Overview of VGG-19:\n",
    "\n",
    "![title](https://miro.medium.com/max/1204/1*9dAefEvl4Oo1QCo_oLAOSw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6xyxnozy8bha"
   },
   "source": [
    "### Content Loss:\n",
    "* Content Loss function makes sure that the content present in the content image (B) is captured in the generated image (C).\n",
    "* It captures the root mean square error between the feature representations in layer I (of generated image and the content image)\n",
    "\n",
    "* Let p be the original image and x be the image generated.\n",
    "* P and F are their respective feature representations in layer <b> l </b>.\n",
    "Thus, the content loss is defined as the squared-error loss between the two feature representation.\n",
    "\n",
    "![title](https://miro.medium.com/max/890/1*LT9330x35To8U5yKhziGrQ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zWYfZQ538hJ9"
   },
   "source": [
    "# Style Loss:\n",
    "\n",
    "* It is defined as the difference of correlation present between the feature maps computed by the generated image and the style image.\n",
    "\n",
    "* If we compute a style matrix for generated and style images, style loss can be seen as the root mean square difference between the two style matrices.\n",
    "\n",
    "\n",
    "\n",
    "* Let a be the original image and x be the the image generated.\n",
    "* Ai and Gi are their respective style representations in layer <b> l </b>.\n",
    "* w is the arbitary weight.\n",
    "\n",
    "The contribution of that layer to the total loss is:\n",
    "\n",
    "![title](https://miro.medium.com/max/778/1*BJq2SJ-5bOW1TJrGUXjJrQ.png)\n",
    "\n",
    "And the total loss is:\n",
    "\n",
    "![title](https://miro.medium.com/max/618/1*nnyQkctkUmO_zZ1DlLDrMg.png)\n",
    "\n",
    "\n",
    "### Gram Matrix:\n",
    "![title](https://miro.medium.com/max/532/1*BcOk_R3Ky2ADbcBmBL5q6A.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q3sagtuc8mlm"
   },
   "source": [
    "### Combined Loss Function:\n",
    "* α = Content Weight\n",
    "* β = Loss Weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5waP7bbK8qAi"
   },
   "source": [
    "![title](https://miro.medium.com/max/1100/1*grlT7LGNDxnyeM4dNDKP5w.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fiOk4Wpy8uYu"
   },
   "source": [
    "## Assisted Practice\n",
    "<b>Problem Statement: </b>\n",
    "Perform a neural style transfer with TensorFlow using VGG-19 model.\n",
    "<br>\n",
    "<br>\n",
    "<b>Objective: </b>\n",
    "Use Tensorflow for neural style transfer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U7yVv2L58z6W"
   },
   "source": [
    "## Assisted Practice\n",
    "\n",
    "\n",
    "<b> Problem Statement: </b> You work in a startup that develops photo editing features for third party apps. You are assigned with a task of creating an editing feature that implements different art styles on people’s faces.\n",
    "\n",
    "<b>Objective:</b> Use PyTorch to build a neural style transfer model.\n",
    "\n",
    "<b>Link to Input Images:</b> https://www.dropbox.com/sh/gggwid6oc4uo9pc/AAB0Qu4_t2KdnFSCIh7u3mTEa?dl=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NJ5E4L23UwsP"
   },
   "source": [
    "## **Object Detection**\n",
    "\n",
    "Note: Before diving into the hands-on, let's explore the pretrained CNN models that we are leveraging.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NrE8dOJuUwsR"
   },
   "source": [
    "### <b> CNN Pretrained models </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xuxMATAtUwsU"
   },
   "source": [
    "## **Success and History**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kZ-4IYh9UwsX"
   },
   "source": [
    "### **Few Popular CNNs**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u9otcVfTspdV"
   },
   "source": [
    "![Pop CNN](https://drive.google.com/uc?id=1diezztFyAx-o28kiVQhb1xf3Kunvznyq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Os4ToCkDUwsa"
   },
   "source": [
    "### **Few CNN Architectures**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-ol6qjjHtJgG"
   },
   "source": [
    "![Architecture](https://drive.google.com/uc?id=1dLRWFA7k6BP4iQV8D6w8PVHObQeV-vUa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I2Th0lAQUwsb"
   },
   "source": [
    "## **AlexNet**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2T1WnAXTUwsc"
   },
   "source": [
    "### **AlexNet Architecture**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A3Bpfkz8Uwse"
   },
   "source": [
    "![CNN](https://drive.google.com/uc?id=1zk-dm9V1VVbrSmXulKtOlR9P8-oVxwk7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cxqUkectUwsf"
   },
   "source": [
    "### **AlexNet Architecture: Layer 1**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7cpsNvw2Uwsg"
   },
   "source": [
    "![Layer1](https://drive.google.com/uc?id=1YFIxNzULxqoTgr1Aw0DoVGFWibbaK7UE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mzrCjNbHwQ-B"
   },
   "source": [
    "![neuron](https://drive.google.com/uc?id=13Sv8mxJ33yoE4P5E3UT_rv_lH0UQiG2V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4DW21RF7Uwsp"
   },
   "source": [
    "## **VGGNet**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9fE0bzqbUwsr"
   },
   "source": [
    "* Seeks to investigate the effect of depth in large scale image recognition\n",
    "* Fixes other parameters of architecture and steadily increases depth\n",
    "* First place in localization(25.3% error), second in classification(7.3% error) in ILSVRC 2014 using ensemble of 7 networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N-8oW2az6X7T"
   },
   "source": [
    "### **Fixed Configuration**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7kL2X_z6Zr9"
   },
   "source": [
    "* Convolutional Layers: 8 to 16\n",
    "* Fully Connected Layers: 3\n",
    "* Stride: 1\n",
    "* ReLu: Follow all hidden layers\n",
    "* Max-Pooling: 2x2 window\n",
    "* LRN: No perceived improvement in performance so, not used\n",
    "* Padding: s/t spatial resolution is preserved\n",
    "* Convolutional filters: Starts from 64, which doubles after each max-pooling layer until 512 \n",
    "* Filter sizes: 3x3, 1x1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oxkz9jBfUwst"
   },
   "source": [
    "#### **Architecture**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ntj1rtnSUwsv"
   },
   "source": [
    "![12](https://drive.google.com/uc?id=111e_zR4ttJmsT3zxxiDBnbzVIy5vmFLl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZfSa4FPB-JYn"
   },
   "source": [
    "### **3x3 Filters**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lawssv9rUwsw"
   },
   "source": [
    " 0\n",
    "-1\n",
    "0<br>\n",
    "-1\n",
    "4\n",
    "-1<br>\n",
    "0\n",
    "-1\n",
    "0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7DuQ83PyUwsy"
   },
   "source": [
    "* Enables deeper architectures\n",
    "* Minimum size required for learning concepts of horizontal, vertical, and blob\n",
    "* Less parameters for same receptive field:\n",
    "  * Receptive field for stack of three 3x3 filters = one 7x7\n",
    "  * With C input and output channels:\n",
    "    * One layer of 3x3 filters: params =3^2 C^2; grows as 0(n^2)\n",
    "    * Three layers of 3x3 filters: params = 3* (3^2 C^2) = 27 C^2\n",
    "    * One layer of 7x7 filters; params = 7^2 C^2 = 49 C^2\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a_VqMtTq-QeM"
   },
   "source": [
    "### **VGGNet: The Core Idea**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mrzSJHps-R1G"
   },
   "source": [
    "VGG network is derived from the idea of deeper networks with smaller filters. Multiple VGGNet architectures range from 11 to 19 layers but have one 3X3 filter that helps in 3X3 CONV operation with periodic pooling throughout the network.\n",
    "\n",
    "[Reference Paper](https://arxiv.org/pdf/1409.1556.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hDCAuWkNUwsz"
   },
   "source": [
    "## **ResNet**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HgOXmbsfUws3"
   },
   "source": [
    "### **Need for a Deeper Network**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x58bp0ivPvYm"
   },
   "source": [
    "* An extremely deep network contains 152 layers\n",
    "* The presence of a residual learning framework eases the training of networks while deeper neural networks are more difficult to train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BazGog1IPxJY"
   },
   "source": [
    "### **Stacking Deeper Layers**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oznt912WUws6"
   },
   "source": [
    "![image13](https://drive.google.com/uc?id=10GrTuunx5Gn_XLLRWiEDpzF4j60V3YzQ)\n",
    "\n",
    "The deeper model (the 56-layer model) performs worse on both training and test errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5XqxrI8MK5Uw"
   },
   "source": [
    "### **Using Skip Connections**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VXVvsCaEUws7"
   },
   "source": [
    "* Take the activation from one layer and feed it into another layer, much deeper into the network\n",
    "* Use layers to fit residual F(x) = H(x) – x, instead of H(x) directly\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fdYfYZXEJYLO"
   },
   "source": [
    "### **Residual Block**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YncAasWEJaPe"
   },
   "source": [
    "![image 14](https://drive.google.com/uc?id=13tMFlF_D_Wm-SRp1NC1AU4KEijfEBWYO)\n",
    "<br><br>\n",
    "* Input x goes through conv-relu-conv series and gives you F(x)\n",
    "* The result is then added to the original input x such that: H(x) = F(x) + x\n",
    "* In traditional CNNs, H(x) would just be equal to F(x)\n",
    "* So, instead of just computing that transformation (straight from x to F(x)), the term that you must add (F(x)) to the input (x) is computed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i8bQPMOtUws9"
   },
   "source": [
    "### **ResNet Features**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q1dXilUFAX71"
   },
   "source": [
    "* Reduces resolution in the first layer therefore, reducing memory consumption\n",
    "* Maintains resolution and channels in each stage\n",
    "* Averages values from each channel fed to the linear classifier\n",
    "* Divides each stage into residual blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VO4Rn-h1AuLC"
   },
   "source": [
    "### **ResNet Architecture**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LQNZspn6Axm1"
   },
   "source": [
    "* Has additional convolutional layer at the beginning\n",
    "* Has stack residual blocks and every residual block has two 3x3 convolutional layers\n",
    "* Does not have FC layers at the end (only FC 1000 to output classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9LWTy27NHfsi"
   },
   "source": [
    "### **ResNet: Experimental Results**\n",
    "Despite having extremely deep architecture (based on layers) as compared to VGG-16 and VGG-19, ResNet model size is much smaller due to global average pooling rather than fully-connected layers.\n",
    "\n",
    "[Reference Paper](https://arxiv.org/abs/1512.03385)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "02VTOQ-6CoLF"
   },
   "source": [
    "## **Transfer Learning (using the pretrained models)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "471ZNrFDCpT3"
   },
   "source": [
    "The core idea of transfer learning is to use a pretrained model to solve the current classification problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gH2jrL_besw9"
   },
   "source": [
    "![image 17](https://drive.google.com/uc?id=1g6e-xj8kcicnkGLIxBz1sOGh98M_U_gj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QawGLJEHDEUl"
   },
   "source": [
    "#### **Feature Extractor**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6YOUB1M1DgfF"
   },
   "source": [
    "* Remove the classification layer in the network\n",
    "* Pass the input data through a pretrained network\n",
    "* Train the above output on traditional ML algorithms\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fkgwluUdDc7a"
   },
   "source": [
    "#### **Fine Tuning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2ZEAArfvDqT2"
   },
   "source": [
    "* Initialize a pretrained network\n",
    "* Update the classification layers from 1000 neurons to number of classes in the input data\n",
    "* Train the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BqtivdmEDwSO"
   },
   "source": [
    "### **Transfer Learning: Considerations**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FFztf6OaEsGo"
   },
   "source": [
    "![image 18](https://drive.google.com/uc?id=1j-sC4CbxpBdVV6mJauBjSOmP3hmDJHWt)\n",
    "\n",
    "<br>\n",
    "    Note: If the input data has no similarity with the ImageNet dataset, you will end up training the entire network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cH06gXs4E7os"
   },
   "source": [
    "### **Transfer Learning Outcomes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G-nNNCkbFdMM"
   },
   "source": [
    "Below is the table illustrating improvements with transfer learning convolutional networks:\n",
    "<br>\n",
    "![19](https://drive.google.com/uc?id=1FHbEJ_0vL2wUte2S5iWRUuX1amnfKXcz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "12yx1TOpE_4P"
   },
   "source": [
    "Below is the table illustrating improvements with fine tuning convolutional networks:\n",
    "<br>\n",
    "![20](https://drive.google.com/uc?id=1jcapMfBUvS0K5vTo7YiDgzaBqAsgYeFr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9SIhAjERFVDf"
   },
   "source": [
    "## **Assisted Practice**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3wyLztf8GeQi"
   },
   "source": [
    "**Problem Statement:** Classify the images of cats and dogs in a dataset using ResNet50.<br>\n",
    "[Dependencies](https://www.dropbox.com/sh/xmgbd6527nj7kcm/AAA6-G_L0tNKaV6psRiO4tI0a?dl=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UUjjOz1dHESo"
   },
   "source": [
    "## **Object Detection**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ojNek_IdGq9W"
   },
   "source": [
    "### **Detection vs. Classification**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gtS1TWlnGtyk"
   },
   "source": [
    "![21](https://drive.google.com/uc?id=17KB_uF2EBwfzg2CO6tGDTbc6OjXE4OEC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eAR2Ij7IG8Gl"
   },
   "source": [
    "### **Real-Time Detection**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hr1bGZo8HTah"
   },
   "source": [
    "![22](https://drive.google.com/uc?id=1J-i5-EMNRoYz7FS4xq8iqBaHH-6Cpn9y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XJkY0zb5HXgb"
   },
   "source": [
    "### **Evaluating a Detector**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YZ1kXNSsHa0X"
   },
   "source": [
    "#### **Test Image**\n",
    "\n",
    "![23](https://drive.google.com/uc?id=1RMwwvO0RFkyPXUJqcm5LRveGUtREnxdd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T4W8Rr1FIE2d"
   },
   "source": [
    "#### **Ideal Detections after 3 Predictions**\n",
    "\n",
    "![24](https://drive.google.com/uc?id=1biOKWUdSdrYz8dWafvIKc51bHI50w2Rk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1or0uF5TIVe8"
   },
   "source": [
    "### **Sort by Confidence**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aTQaJ67IIuNT"
   },
   "source": [
    "![25](https://drive.google.com/uc?id=1ura-1pqledPDOybRdE8YFI1FjT9P_b9s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4Iz5-feWJWVX"
   },
   "source": [
    "### **Evaluation Metric**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HsQkRkCaJct8"
   },
   "source": [
    "![26](https://drive.google.com/uc?id=12NIfwtgf3Q569qCm50SJvDO_lEPFkX5o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "opXJ-mS5KsC2"
   },
   "source": [
    "![27](https://drive.google.com/uc?id=1AjD78EQmYEUOSW6WAnGzNkgiNt4kkW7f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vEyZhup11cBB"
   },
   "source": [
    "## **IOU (Intersection Over Union)**\n",
    "\n",
    "* An advanced metric for object detection\n",
    "* A measurement for determining the overlap between two areas (in case of computer vision it is between two images).\n",
    "* Measures how two areas are equal in terms of size and location of the area \n",
    "* If two areas are exactly equal, IOU will be 1\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "![IOU](https://drive.google.com/uc?id=1nFFHg88D7osq-QZ0HG1z3PtOmLg8CyA4)\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "## **Implementing IOU**\n",
    "\n",
    "The bounding box coordinates are in the form (x, y, width, height). You will first calculate the width and height of the Intersection Box and size of Intersection will be area of the Intersection Box. You can get the Union size by subtracting the Intersection size from total area.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mOmbpYoaCzc6"
   },
   "source": [
    "### Function For IOU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Na92zk-r92fj"
   },
   "outputs": [],
   "source": [
    "def IOU(box1, box2):\n",
    "    x1, y1, w1, h1 = box1\n",
    "    x2, y2, w2, h2 = box2\n",
    "    w_intersection = min(x1 + w1, x2 + w2) - max(x1, x2)\n",
    "    h_intersection = min(y1 + h1, y2 + h2) - max(y1, y2)\n",
    "    if w_intersection <= 0 or h_intersection <= 0: # No overlap\n",
    "        return 0\n",
    "    I = w_intersection * h_intersection\n",
    "    U = w1 * h1 + w2 * h2 - I # Union = Total Area - I\n",
    "    return I / U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jgew78Eo-fUF"
   },
   "source": [
    "## Assisted Practice\n",
    "Problem Statement: Generate a random rectangle, detect the object using Keras, and evaluate IOU for the prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O5BT4gLdMHBn"
   },
   "source": [
    "## **YOLO (You Only Look Once)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3NgyGfdXMN1O"
   },
   "source": [
    "### **YOLO Features**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cVbq6v0OMSq2"
   },
   "source": [
    "* Generalized learning\n",
    "* Exceptionally fast\n",
    "* Processes the entire image simultaneously\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dGwrGxOLMYvg"
   },
   "source": [
    "### **YOLO Detection**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mkNnkat8W6jT"
   },
   "source": [
    "### **Steps Performed in YOLO Detection**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7lt4zgPcSOc-"
   },
   "source": [
    "* Consider the below image of a dog and a bicycle which will be fed to the YOLO algorithm.\n",
    "\n",
    "![28](https://drive.google.com/uc?id=103cEhMpKrFuMxJvhoZ233RBnhwHc12Mq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S4mvuj5ASgLr"
   },
   "source": [
    "* The image gets split into an S*S grid.\n",
    "\n",
    "\n",
    "![29](https://drive.google.com/uc?id=1a4zG9jZfyLbGUD4DzaltcznUMPICQlbP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7NXZvgoCS53H"
   },
   "source": [
    "* Each cell contributes to B bounding boxes (x,y,w and h)\n",
    "\n",
    "![30](https://drive.google.com/uc?id=1uev21zfH_rs7qOIL7oKen-rwXgl7QbLx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2gQVVnxHSyZr"
   },
   "source": [
    "* Confidences are assigned to each of the bounding boxes (x,y,w and h)\n",
    "\n",
    "![31](https://drive.google.com/uc?id=1NBDSA1PkfPqksZeb2yRt8c565xz8vDDB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tmhl9aAgTaRc"
   },
   "source": [
    "* The above steps continue simultaneously for every cell.\n",
    "\n",
    "![31](https://drive.google.com/uc?id=1xIkOfZ-ituKySu4dP9_S1_sXWcn5QM14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O_l_6CZQT0JW"
   },
   "source": [
    "* Apart from bounding boxes, each cell also contributes to class probabilities for individual bounding box.\n",
    "\n",
    "![33](https://drive.google.com/uc?id=1himJKpWL-bQmDJDt6my2effL80-yK2NV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sX_qm7biUMD2"
   },
   "source": [
    "* Conditional probability is evaluated based on the corresponding object.\n",
    "\n",
    "![32](https://drive.google.com/uc?id=1rTUrBm_2FlDYMiXRXb28DSia100673xr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SUTOJ1irUYEa"
   },
   "source": [
    "* The box and class predictions are then combined.\n",
    "\n",
    "![35](https://drive.google.com/uc?id=100MIN2m1tnPr4H6eUun-P4DwsJ2eLoZl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fdNUm2VIUHS3"
   },
   "source": [
    "* Finally, you proceed with threshold detections.\n",
    "\n",
    "![36](https://drive.google.com/uc?id=1abgo0VB6IPKgI_wQn1iyr8ltKjgxEfYn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qoOPoD1QU7Yb"
   },
   "source": [
    "## **Implementing YOLO**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iG_fjHm2U-SZ"
   },
   "source": [
    "### **Training Phase Steps**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R9qc_OecXEQo"
   },
   "source": [
    "* While training your YOLO model, you must match objects to the right cells.\n",
    "\n",
    "![37](https://drive.google.com/uc?id=1pwA2u0WykOrZx3afhy4mKMiJ7zcX3Wdc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gNEo1gwuXICS"
   },
   "source": [
    "* You should alter the class prediction for the cell.\n",
    "\n",
    "![37](https://drive.google.com/uc?id=10yCMhbj9A1qzMsv8m8cGN1sZKf2lHZue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pGbhyiD_XuRH"
   },
   "source": [
    "* Also, examine the boxes predicted by the cell.\n",
    "\n",
    "![39](https://drive.google.com/uc?id=1Su3Pk_phnb7n6ADod5pKuL0hwilxy84x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OrL_2KipX--U"
   },
   "source": [
    "* You must adjust the boxes with a specific confidence for each of them and increase the confidence accordingly.\n",
    "\n",
    "![40](https://drive.google.com/uc?id=1dZXhPvDJMENtoBeWtIFBkiE1ufvRUI46)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BsdCjjq-YTAM"
   },
   "source": [
    "* Also, decrease the confidence for other boxes.\n",
    "\n",
    "![41](https://drive.google.com/uc?id=1PMSVtwNDUQSMKBYDWO2TZFpocS-GtBBO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OtvOX-4pYr_7"
   },
   "source": [
    "* You may end up with some cell predictions which lack true detections.\n",
    "\n",
    "![42](https://drive.google.com/uc?id=1q0xNPUOWSGtRyrbyyF4y3T3Texz93FtB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8WlZ1WbfY8DT"
   },
   "source": [
    "* Therefore, decrease the confidence for such cells\n",
    "\n",
    "![43](https://drive.google.com/uc?id=1WE8kwbT1JqLnOKX4baPLtGFoyYCZQ1zw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bl2i7mjRwDUR"
   },
   "source": [
    "### Advancements in YOLO:\n",
    "* <b> YOLOv2 </b> uses a few tricks to improve training and increase performance. Like Overfeat and SSD we use a fully-convolutional model, but we still train on whole images, not hard negatives. Like Faster R-CNN we adjust priors on bounding boxes instead of predicting the width and height outright. However, we still predict the x and y coordinates directly.\n",
    "* <b> YOLOv3 </b> uses a few tricks to improve training and increase performance, including: multi-scale predictions, a better backbone classifier, and more. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nvnmB87MpNVr"
   },
   "source": [
    "### **Assisted Practice**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bGNmRD2upRd_"
   },
   "source": [
    "**Problem Statement:** Detect objects in the image using pretrained YOLO on COCO dataset that consists of 80 objects with labels.\n",
    "<br>\n",
    "[Dependencies](https://www.dropbox.com/s/3sc7bkculbkm8n2/Dependencies-20200418T111606Z-001.zip?dl=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q2OwqSvbpgx_"
   },
   "source": [
    "### **Knowledge Check**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qMzv1SK4pkhe"
   },
   "source": [
    "\n",
    "Click [here](https://drive.google.com/open?id=1ABWWW-AGhvbPq9MJ5Z17rAe2uLWjqNvX) for knowledge checks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x9B26P-J2A7R"
   },
   "source": [
    "## **Key Takeaways**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4qaR3EV82Dh0"
   },
   "source": [
    "* Neural style transfer takes two images as input.\n",
    "* Color, texture, etc. are styles extracted from the input called style image.\n",
    "* Shape, design, etc. are content properties extracted from the input called content image.\n",
    "* VGGNet investigates the effect of depth in large-scale image recognition where it fixes other parameters of architecture and steadily increases the depth.\n",
    "* ResNet reduces the resolution in the first layer therefore, reducing memory consumption. It also maintains the resolution and channels in each stage, where average values from each channel are fed to the linear classifier.\n",
    "* YOLO exhibit is exceptionally fast because it processes the entire image simultaneously and also shows generalized learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IegFo7xruhFa"
   },
   "source": [
    "### Problem Statement:\n",
    "You are provided with a trained model of YOLO v3 on the MSCOCO dataset. Using this model, you have to create an object detection program for the different objects of the dataset.\n",
    "### Objective:\n",
    "Use YOLO v3 pretrained model for object detection.\n",
    "\n",
    "Link to Dependencies: https://www.dropbox.com/sh/b8zuyzbiey8kdso/AAAVKIorrhF-khmfNAFdSbhba?dl=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2A_qgiD0vGPH"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lesson 3_Applications: Neural Style transfer and Object Detection.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
